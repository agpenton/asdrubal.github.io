[
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/lambdas/image-resize/",
	"title": "Image resize",
	"tags": [],
	"description": "",
	"content": "Here is the documetation for all the Lambda system used in Juniqe Shop.\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/",
	"title": "Infrastructure",
	"tags": [],
	"description": "",
	"content": "Chapter 1 Basics Discover what this Hugo theme is all about and the core concepts behind it.\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/",
	"title": "Landing Page",
	"tags": [],
	"description": "",
	"content": "DevOps Documentation This Website will host the documetation used for the DevOps process for the Juniqe Infrastructure.\nThe Content of this website is:\nInfrastructure DevOps Entrypoints Cloud \u0026amp; Infrastructure Descripttion Infrastructure Documentation "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/infrastructure-docs/",
	"title": "Infrastructure Documentation",
	"tags": [],
	"description": "",
	"content": "Chapter 2 Infrastructure Documentation "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/onboarding/",
	"title": "Onboarding",
	"tags": [],
	"description": "",
	"content": "New Accounts and [on/off] boarding "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/applications/",
	"title": "Applications",
	"tags": [],
	"description": "",
	"content": "Chapter 3 Applications Here is the documetation for all the Applications used in Juniqe Shop.\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/lambdas/",
	"title": "Lambda System",
	"tags": [],
	"description": "",
	"content": "Chapter 4 Lambda System Here is the documetation for all the Lambda system used in Juniqe Shop.\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/howto/",
	"title": "Howto",
	"tags": [],
	"description": "",
	"content": "Chapter 5 How-TO Chapter to hots all the howtos for any kind of proccess.\nRequirements. kubectl helm pulumi terraform aws cli sam cli python 3 typescript golang nodejs docker docker-compose "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/infrastructure-docs/authentication/",
	"title": "Authentication",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/infrastructure-docs/cloud/",
	"title": "Cloud Documentation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/onboarding/providers/",
	"title": "Providers",
	"tags": [],
	"description": "",
	"content": "New Accounts and [on/off] boarding "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/monitoring/",
	"title": "Monitoring",
	"tags": [],
	"description": "",
	"content": "Chapter 6 Monitoring System. Lorem Ipsum.\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/troubleshooting/",
	"title": "Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Chapter 7 Troubleshooting Troubleshooting issues in the infrastructure.\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/howto/pulumi/",
	"title": "Pulumi",
	"tags": [],
	"description": "",
	"content": "Sub Chapter 5.1 This sub-chapter will contain the themes relatedd to Pulumi operations.\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/troubleshooting/performance/",
	"title": "Performance",
	"tags": [],
	"description": "",
	"content": "How to address issues performance issues Use this diagram to solve issues\nhttps://drive.google.com/file/d/1WaTRu9OIoUy-4a9XbtqVbpYQpUpsfGoP/view?usp=sharing\nHow to check if not enough resources Memory: OOM events, memory usage is close to maximum, gc behaviour CPU: High cpu usage(more or close then maximum) Space: Service is non fuctional or throws errors on IO\nCheck if we have more traffic then usually Open Traffic rate grafics retrospectively for 1h, 12h, 2 days, 7 days windows and search for anomalies\ncheck if traffic spike is correlating with perormance issues Look for spike window and correlate with reponse time, error rates\nCheck errors Check error rates on grafana metrics, for 5xx on shop and magento check kibana vis table for exact uri\nelk.intra.juniqe.com/_plugin/kibana/app/kibana#/visualize/edit/e71b0680-2361-11eb-bf46-11db10131e73?_g=h@44136fa\u0026amp;_a=h@518cb23\ncheck sentry, kibana app logs\nIs service in process of scaling k8s deployment For example for shop, check \u0026ldquo;Deployment pods\u0026rdquo;\nkubectl describe hpa shop-app\ncurrent should be less then desired and going up\nDeployment pods: 5 current / 5 desired Also look at events field for scaling events or lack of them.\nor check grafana dashboard for such metric\nECS service Go to ECS console, open \u0026ldquo;backend\u0026rdquo; ecs cluster, find service and check for running / desired instances. Also check events tab for scaling events\nor check datadog dashboard for such metric\nCluster has enough capacity for new instances ? Check events for instance allocation errors\nkubectl describe hpa shop-app Open AWS/EC2/Autoscaling page and check events tab for autoscaling events. There are should be no errors. Pools should not be at maximum\nActions Vertical scaling (more resources) k8s deployment Find k8s deployemnt resource in k8s definition and change resources specification.\nresources: requests: memory: \u0026#34;2600Mi\u0026#34; cpu: \u0026#34;2048m\u0026#34; limits: memory: \u0026#34;2600Mi\u0026#34; cpu: \u0026#34;4096m\u0026#34; run skaffold deploy -p juniqe-production --images \u0026quot;\u0026lt;current image tag\u0026gt;\u0026quot;\nECS Find taskdefinition (for example juniqe-infra/terraform/modules/juniqe/ecs-shop/files/taskdefinition.json)\nchange memory, memoryReservation, cpu.\nRun terraform init, terraform apply for this service\nHorizontal scaling (more instances) k8s deployement Find k8s HorizontalPodAutoscaler resource and change maximum and minimum replicas\nrun skaffold deploy -p juniqe-production --images \u0026quot;\u0026lt;current image tag\u0026gt;\u0026quot;\nEcs Find service layer in juniqe-infra/terraform/envs and change maximum/minimum in service module.\nMain services Storefront CDN Main content distribution. Proxies to nginx-frontend(shop,checkout) and pdp-app.\nCaches /, /api/, /_next/ and pdp-app pdp pages.\nMetrics https://grafana.prod.juniqe.com/d/D9AMi9hGk/storefront-cdn?orgId=1\nCheck error rates\nDependencies nginx-frontend pdp-app Nginx-frontend Ingests and routes traffic between shop and magento\nMetrics https://app.datadoghq.com/dashboard/628-s9p-pg2/nginx-frontend?from_ts=1605024289396\u0026amp;live=true\u0026amp;to_ts=1605027889396\u0026amp;tpl_var_env=juniqe-production\ncheck cpu, amount of containers\nLogs Metrics https://app.datadoghq.com/dashboard/628-s9p-pg2/nginx-frontend?from_ts=1605024289396\u0026amp;live=true\u0026amp;to_ts=1605027889396\nDependencies shop-app magento-checkout Shop-app Metrics check response time, error rates\nhttps://grafana.prod.juniqe.com/d/HlhbEnKMz/shop-app?orgId=1\nManual check Page is opening https://www.juniqe.de/wandbilder/poster\nMain page is cached, apis are cached\nDependencies Masterdb Magento Internal ES Storage Caches (Redis \u0026amp; Memcached) DY PDP-APP Metrics What to check: response time, error rates\nhttps://grafana.prod.juniqe.com/d/FYp22acGz/pdp-app?orgId=1\nManual check pdp pages are all cached\nYou could try to open this pathes, which are not blocked http://pdp-app.prod.juniqe.com/status\nhttp://pdp-app.prod.juniqe.com/metrics\nDependencies shop-app Magento Checkout serves /checkout, payment provider callbacks\nMetrics Check 5xx error rates, listen queue size, request times, container count\nhttps://app.datadoghq.com/dashboard/j2s-r88-9gb/magento-checkout\nManual check Open Page https://www.juniqe.de/checkout/onepage/ or place an order with Advanced Payment\nDependencies masterdb slavedb memcached (sessions) redis (configuration) Magento Internal serves /reboot/ endpoints which are used in shop /cart/add /cart/get and session generation.\nMetrics Check 5xx error rates, listen queue size, request times, container count\nhttps://app.datadoghq.com/dashboard/i9n-aph-uyk/magento-internal\nManual check Open https://www.juniqe.de/reboot/session/init Should generate new session if there are no frontent session cookie\nDependencies masterdb slavedb memcached (sessions) redis (configuration) Master Database Metrics https://grafana.prod.juniqe.com/d/7afX4nKGz/amazon-mysql-rds?orgId=1\nCheck CPU, Burst Balance\nManual check Connect to database\nmysql -h masterdb.juniqe-production run select now()\nCheck processlist for long queries\nselect ID,USER,TIME,STATE from information_schema.processlist where COMMAND!=\u0026#34;Sleep\u0026#34; order by TIME\\G Vertical scaling In ~/workspace/juniqe-infra/terraform/envs/juniqe-production/env.auto.tfvars change \u0026ldquo;instance.types.rds\u0026rdquo; and apply\nSlave database Special replica is used only during blackfriday to unload some read only magento checkout queries.\nMetrics https://grafana.prod.juniqe.com/d/7afX4nKGz/amazon-mysql-rds?orgId=1\nTo find slow queries, this tool can be used AWS performance insights https://eu-central-1.console.aws.amazon.com/rds/home?region=eu-central-1#performance-insights:resourceId=db-OMUTTKKBJDQDDHOEOYI3W66D2I;resourceName=masterdb\nCheck Replica Lag, CPU, Burst Balance\nVertical scaling In ~/workspace/juniqe-infra/terraform/envs/juniqe-production/env.auto.tfvars change \u0026ldquo;instance.types.rds\u0026rdquo; and apply\nHorizontal scaling In ~/workspace/juniqe-infra/terraform/envs/juniqe-production/rds-slavedb/main.tf\nadd new slave module with uniqe name (slavedb-02,03..etc). make sure load_weight is set to 0, as we do not want to send traffic to not warmed up instance.\nrun terraform apply\nAfter successfull apply (spinning new instance will take 10-20 min), start increasing load_weight gradually with 10-20 step each 10-20 min.\nMonitor cpu and query latency, query distribution skew between slaves\nManual check Connect to database\nmysql -h slavedb-01.juniqe-production Check processlist for long queries, or if there are any queries\nselect ID,USER,TIME,STATE from information_schema.processlist where COMMAND!=\u0026#34;Sleep\u0026#34; order by TIME\\G check lag\nSHOW SLAVE STATUS\\G There are should be no errors, Seconds_Behind_Master: 0 and Slave_SQL_Running: Yes\nany lag can stop checkout from happening. there are automation to route queries to master\ncheck current routing:\ndig default.rr.readonly-slavedb.juniqe-production\nas this record is server side round robin, it can return different result each run if weight is not 0% or 100%\nElasticSearch Storage Metrics Check:\ncluster status, shards in non active state should be zero (affecting shop availability) threadpools \u0026amp; queues (large queue could mean long shop api response) thread pool rejections ( non zero could mean failed queries) heap https://grafana.prod.juniqe.com/d/n_nxrE_mk/juniqe-storage-elasticsearch?orgId=1\u0026amp;refresh=1m\u0026amp;var-env=juniqe-production\nManual check Check cluster status is green:\nhttp://storage6-01.juniqe-production:9200/_cluster/health\nCheck which indice is broken: http://storage6-01.juniqe-production:9200/_cat/indices?v\nCheck nodes status: http://storage6-01.juniqe-production:9200/_cat/nodes?v\nUse ES head chrome extension to debug cluster and indices health https://chrome.google.com/webstore/detail/elasticsearch-head/ffmkiejjmecolpfloofpjologoblkegm\nHorizontal scaling in \u0026lsquo;~/workspace/juniqe-infra/terraform/envs/juniqe-production/storage/main.tf\u0026rsquo;\nchange elasticsearch_cluster_size (on-demand instances) or elasticsearch_cluster_spot_size.\nrun terraform apply\nDo not add too much instances at once.\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/monitoring/system/",
	"title": "System",
	"tags": [],
	"description": "",
	"content": "Tools Grafana Grafana https://grafana.prod.juniqe.com/d/XjxcnIcMk/juniqe?orgId=1\ngrafana is main tool for dashboarding and alerting. Grafana uses prometheus, aws cloudwatch, aws X-ray, Kibana and other datasources as timeseries source to draw panels and fire metircs.\nMain datasource is named \u0026ldquo;Prometheus thanos\u0026rdquo;. This datasource provides most of the metrics via PromQL api and combines prometheus metrics from juniqe-staging and juniqe-production.\nPromQL - prometheus Query language. We have to use PromQL to query prometheus timeseries metric storage, configure panels.\nExamples:\nReturn all time series with the metric http_requests_total:\nhttp_requests_total\nReturn all time series with the metric http_requests_total and the given job and handler labels:\nhttp_requests_total{job=\u0026quot;apiserver\u0026quot;, handler=\u0026quot;/api/comments\u0026quot;}\nReturn a whole range of time (in this case 5 minutes) for the same vector, making it a range vector:\nhttp_requests_total{job=\u0026quot;apiserver\u0026quot;, handler=\u0026quot;/api/comments\u0026quot;}[5m]\nNote that an expression resulting in a range vector cannot be graphed directly, but viewed in the tabular (\u0026ldquo;Console\u0026rdquo;) view of the expression browser.\nUsing regular expressions, you could select time series only for jobs whose name match a certain pattern, in this case, all jobs that end with server:\nhttp_requests_total{job=~\u0026quot;.*server\u0026quot;}\nAll regular expressions in Prometheus use RE2 syntax.\nTo select all HTTP status codes except 4xx ones, you could run:\nhttp_requests_total{status!~\u0026quot;4..\u0026quot;}\nDocs:\nQuery Basics: https://prometheus.io/docs/prometheus/latest/querying/basics/ Cheat Sheet: https://promlabs.com/promql-cheat-sheet/ PromQL in Grafana: https://grafana.com/docs/grafana/latest/datasources/prometheus/ To get list available metrics, got to explore https://grafana.prod.juniqe.com/explore and start typing roughly what you need. Grafana will suggest possible metrics.\nwell-known metrics prefixes aws_ - imported AWS cloudwatch metrics aws_lambda_ - aws lambda metrics aws_sqs_ - aws sqs metrics prometheus_ - prometheus internal metrics juniqe_ - Custom juniqe app metrics juniqe_shop_ - custom juniqe_shop metrics juniqe_ct_ - commercetools imported metric (funnel-monitoring lambda) http_ - http services counters, histograms (also used in juniqe apps) kube_ - k8s related metrics container_ - container related metrics well-known metrics labels env - juniqe Environment. Value can be juniqe-staging or juniqe-production namespace - kubernetes namespace name - exact ID of entity tag_Something - imported from aws resource tag. It is preffered to use tags over IDs or names or ARNs Sentry https://sentry.io/organizations/juniqe/projects/\nSentry is application error managing tools with smart event merging capabilities. Every app has it\u0026rsquo;s own project in sentry. Sentry usually only recieve error or higher level events such as logged errors and exceptions.\nELK aka Kibana https://elk.intra.juniqe.com/_plugin/kibana/app/kibana#/home?_g=()\nKibana contains:\nmostly all logs from all applications(except lambdas) nginx access logs Kibana implements easy to use logs filtering and analyzing solution\nSame data is avaliable in Grafana from elasticsearch datasource\nAWS cloudwatch AWS monitoring solution. Provides many different products for metrics, logs and tracing\nCan be accessed via https://eu-central-1.console.aws.amazon.com/cloudwatch/home?region=eu-central-1\nFull lambda log, eks plane logs can be found in cloudwatch logs\nLogs and metrics are available from AWS cloudwatch datasource. Metrics from datasource are lack of tags.\nMetrics are also available from prometheus datasource under aws_ prefix.\nAWS Xray AWS application tracing solution. Mainly used in lambdas to trace performance.\nX-ray is available in grafana as datasource\nMorning triage workflow Triage new errors in grafana sentry and anomalies in grafana before standup Sentry: Open your project with filters Environment=juniqe-production time=24h (or time since last triaging) with sort by frequency and lookup for new err without assignee and issue Look at grafana dashboard if there are some possible anomaly Give team a small overview about new issues in your scope on standup New errors or High frequent errors Alerts Anomlies create tickets on issus if needed Solve issue if it\u0026rsquo;s High freq, user facing error. possible high conversion decrease or notify person who can do that Apply hotfix and close issue What to do with alert notify team that you are checking this problem Go to Grafana Panel Dashboard by link in Alert Check if this error creates problems for end users Check for other symptoms on Dashboard (RPS, Errors) Check logs in sentry, cloudwatch(available in grafana) Find root cause, create an issue Start working fix if this is urgent Application - person responsibility matrix How to use this: put 1 for first responsibilty and 2 for second. Check checksums on the edges. For each project put dashboard link and sentry link\nhttps://docs.google.com/spreadsheets/d/1EXsZjphL7Yc5_-t6IWTS05lNYJg_sJQMqSDkAXQ9nrw/edit#gid=0\nHow to monitor Ensure that application have sentry integrated, put Link to sentry into Application - person responsibility matrix doc and Readme.md of project Ensure that k8s service application has prometheus /metrics published (scala, nodejs apps) and PodMonitor k8s resouce is configured Ensure that lambda has X-ray integrated Ensure that lambda stack resources (lambda, sqs, api gw), have tag Name (they would not appear in prometheus otherwise) Create grafana dashboard. Examples Lambda: https://grafana.prod.juniqe.com/d/i0_BHJ5Gz/cart-public-api-lambda?orgId=1 k8s service app: https://grafana.prod.juniqe.com/d/HlhbEnKMz/shop?orgId=1 Which alerts to create and what monitor Alert on High Latency Mandatory alert\nReflects resposivness of service.\nExamples: Execution time of lambda http response time latency How to set threshold: 500ms-1000ms - services involved in customer engagement (shop, category, pdp-app, etc), before conversion. 1000ms-5000ms - services after conversion: checkout, returns\nExact value should be set by historical p90 or p95 when service was normally functioning.\nAlternatively, better practice is to use advanced APDEX score: https://prometheus.io/docs/practices/histograms/#apdex-score https://en.wikipedia.org/wiki/Apdex\nAlert on number of Errors Mandatory alert\nAmount of errors. This metric describes overall health of application\nExamples: lambda not successful invocations 5xx errors 4xx errors How to set threshold Best case scenarion alert has to be set for max(errors{5m}) \u0026gt; 0. If app throws errors as part of normal functioning (404 not found, intermediate errors), threshold should be set for historical p95.\nAlternatives:\nCalcualte percent of errors Include errors in APDEX score as Frustrated Alert on number of requests Not Mandatory alert.\nAmount of requests.\nExamples: http requests per second(hour,minute) to application new messages in sqs per second How to set threshold historical maximum * 2\nAlert on resource saturation Not Mandatory alert\nAim to monitor and track the usage of every resource the service relies upon. Some resources have hard limits you cannot exceed, like RAM, disk, or CPU quota allocated to your application. Other resources—like open file descriptors, active threads in any thread pools, waiting times in queues, or the volume of written logs—may not have a clear hard limit but still require management.\nDepending on the programming language in use, you should monitor additional resources:\nIn Java: The heap and metaspace size, and more specific metrics depending on what type of garbage collection you’re using In Go: The number of goroutines The languages themselves provide varying support to track these resources.\nWhen the resource has a hard limit When crossing a usage threshold causes performance degradation You should have monitoring metrics to track all resources—even resources that the service manages well. These metrics are vital in capacity and resource planning.\nExamples:\nLambda throttles Disk usage How to set threshold 5-10% of total resource\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/troubleshooting/staging/",
	"title": "Staging",
	"tags": [],
	"description": "",
	"content": "Solving staging problems Master is deployed and it\u0026rsquo;s not working Check #dev-team channel if there announcement about someone working on staging. If none, say \u0026ldquo;Component X is not working on staging, feature X is not working for me. Master is deployed\u0026rdquo; Proceed to debug with \u0026ldquo;Known Issues\u0026rdquo; section Check sentry for errors in broken component Check kibana for errors in broken component (filter by app name) Check kubectl logs for errors in broken component (kubectl logs deploy/shop-app \u0026hellip; etc ) Check Grafana dashboard for component on staging (set env to juniqe-staging) CHeck last deployment job in gitlab for errors Check AWS health dashboard if there are ongoing incidents You deployed some branch, it does not work deploy master If deployment fails with errors \u0026ldquo;Waiting for pod default/runner-ldomxexz-project-13374321-concurrent-0wzvvr to be running, status is Pending ERROR: Job failed (system failure): prepare environment: timed out waiting for pod to start. Check https://docs.gitlab.com/runner/shells/index.html#shell-profile-loading for more information\u0026rdquo;. follow \u0026ldquo;Gitlab CI Runner is not running\u0026rdquo;. Check if it\u0026rsquo;s working If master does not work, follow \u0026ldquo;Master is deployed and it\u0026rsquo;s not working\u0026rdquo; If master is working, sync your branch with master and then deploy You have not deployed anything and it does not work Check if master is deployed If not, Check #dev-team channel if there announcement about someone working on staging. If not follow \u0026ldquo;You deployed some branch, it does not work\u0026rdquo; \u0026ldquo;Gitlab CI Runner is not running\u0026rdquo; Try to restart Pipeline/job later Check if other pipelines are also failing How long pipelines are failing? Check if there are many schedulle failures kubectl get events -A | grep FailedScheduling Check if k8s have schedullable nodes. kubectl get node | grep Ready should show nodes (5-10 usually, not 0 and not 1) If yes, proceed with k8s cluster cannot schedulle pods on nodes k8s cluster cannot schedulle pods on nodes Check if spot.io controller is connected to spot.io. https://console.spotinst.com/spt/ocean/aws/kubernetes/view/o-fcc91800 Should not show any errors Check spot.io Log tab for error \u0026ldquo;03/18/2021, 08:19:09, WARN, Auto-scaling activity is currently suspended due to a cluster configuration error. The spotinst cluster controller has not reported a heartbeat to the spotinst API.\u0026rdquo; If there are such error, check if controller is working kubectl -n kube-system get deploy/spotinst-kubernetes-cluster-controller should show ready status 1/1 Check logs for errors kubectl -n kube-system logs deploy/spotinst-kubernetes-cluster-controller | grep -v INFO Try to reload spot.io controller kubectl -n kube-system rollout restart deploy/spotinst-kubernetes-cluster-controller If nothing helps, contact spot.io support. Known Issues Shop project was freshly deplyoyed after nginx (blue whale on all pages) This could happen if nginx was deployed before shop ingress\nCheck if staging.juniqe.com is showing blue whale on all pages(not only catalogue) restart nginx-fronted service in AWS ECS console (ECS-\u0026gt;backend-staging-\u0026gt;services-\u0026gt;nginx-frontend-\u0026gt;update-\u0026gt;force new deploy-\u0026gt;skip to review-\u0026gt; apply ) Shop Main page opens, but catalogue links are broken (blue whale) Check sentry for errors like this\nUnknownHostException com.juniqe.storage.rest.RestStorage in $anonfun$performRequest$1 error storage6-01.juniqe-stagingcontrollers.shop.HealthController search link https://sentry.io/organizations/juniqe/issues/?environment=juniqe-staging\u0026amp;project=1375821\u0026amp;query=is%3Aunresolved+storage\u0026amp;statsPeriod=24h\nif storage cannot be reached, check\nES storage is deployed, healthy, has indices and scripts app configuration is right "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/howto/cloud/",
	"title": "Cloud",
	"tags": [],
	"description": "",
	"content": "Monitoring Using Staging Solving staging issues Kubernetes Tooling To work with AWS EKS Kubernetes you will need to install this tools:\ninvoke command line utility to run our make-like tasks. This step is optional. Learn more\ndocker https://docs.docker.com/install/\naws cli https://aws.amazon.com/cli/\naws-iam-authenticator https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html\namazon-ecr-credential-helper https://github.com/awslabs/amazon-ecr-credential-helper\naws-google-auth https://github.com/cevoaustralia/aws-google-auth\nkubectl https://kubernetes.io/docs/tasks/tools/install-kubectl/ skaffold https://skaffold.dev/docs/install/ kustomize https://github.com/kubernetes-sigs/kustomize kubeseal https://github.com/bitnami-labs/sealed-secrets/releases (Optional) Preparation Following steps are described for AWS juniqe-staging account\nAWS auth We need to authorize in AWS to\nGet auth tokens for kubectl with aws-iam-authenticator to push docker images artifacts Elastic container Registry amazon-ecr-credential-helper Setting up AWS profile first time run Authorization in AWS is done via aws-google-auth, as our google account have mapping to aws roles\nOpen or create ~/.aws/config file with following contents or add to existing one:\n[profile juniqe-staging] region = eu-central-1 google_config.ask_role = False google_config.keyring = False google_config.duration = 42000 google_config.google_idp_id = C00wp1uuf google_config.google_sp_id = 86281751805 google_config.u2f_disabled = False google_config.google_username = replace-username-with-yours@juniqe.com google_config.role_arn = arn:aws:iam::445858552116:role/juniqe-staging/admin google_config.bg_response = None for production env:\n[profile juniqe-production] region = eu-central-1 google_config.ask_role = False google_config.keyring = False google_config.duration = 42000 google_config.google_idp_id = C00wp1uuf google_config.google_sp_id = 86281751805 google_config.u2f_disabled = False google_config.google_username = replace-username-with-yours@juniqe.com google_config.role_arn = arn:aws:iam::054846004576:role/juniqe-production/operator google_config.bg_response = None Make sure you\u0026rsquo;ve replaced replace-username-with-yours@juniqe.com with yours juniqe email\nNow run aws-google-auth command to authorize yourself in AWS via google account\naws-google-auth -d 42000 -p juniqe-staging --role-arn arn:aws:iam::445858552116:role/juniqe-staging/admin for production env\naws-google-auth -d 42000 -p juniqe-production --role-arn arn:aws:iam::054846004576:role/juniqe-production/operator By setting this env variable we will switch current aws profile to juniqe-staging. We should use it before running any AWS call\nexport AWS_PROFILE=juniqe-staging For production, set to juniqe-production\nexport AWS_PROFILE=juniqe-production To check if you are authorized in aws\naws sts get-caller-identity You should get json with your account information\n{ \u0026#34;UserId\u0026#34;: \u0026#34;AROAI4JI2N52XMITA3AIC:marat@juniqe.com\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;445858552116\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:sts::445858552116:assumed-role/admin/marat@juniqe.com\u0026#34; } Confluence has more extensive info https://juniqe.atlassian.net/wiki/spaces/EN/pages/65568769/AWS+Credentials+via+SAML+SSO\nECR authorization To authorize in ecr we need aws-iam-authenticator and docker helper configuration\nOpen ~/.docker/config.json and add credHelpers object to json:\n{ \u0026#34;credHelpers\u0026#34;:{ \u0026#34;603482864741.dkr.ecr.eu-central-1.amazonaws.com\u0026#34;:\u0026#34;ecr-login\u0026#34;} } This will enable ECR auth trough amazon-ecr-credential-helper\nAWS EKS K8S authorization first time run To authorize in K8S we have to provision KUBECONFIG file.\nList EKS clusters available in AWS account\naws eks list-clusters { \u0026#34;clusters\u0026#34;: [ \u0026#34;kjsmain-eksCluster-d71139f\u0026#34;, ## old 1.14 cluster \u0026#34;juniqe-eksCluster-4e5ef16\u0026#34; ## new 1.18+ cluster ] } Update your KUBECONFIG with new cluster:\naws eks update-kubeconfig --name juniqe-eksCluster-4e5ef16 --alias juniqe-staging for production\naws eks update-kubeconfig --name juniqe-eksCluster-e5415f2 --alias juniqe-production Switch to context and change default namespace\nkubectl config use-context juniqe-staging kubectl config set-context --current --namespace=juniqe-staging Switch to context and change default namespace\nkubectl config use-context juniqe-production kubectl config set-context --current --namespace=juniqe-production Now we can check if we have access to namespace:\nkubectl get namespace NAME STATUS AGE default Active 197d gitlab Active 183d juniqe-integration Active 197d juniqe-staging Active 197d kube-node-lease Active 197d kube-public Active 197d kube-system Active 197d Get pod list\nkubectl get pod NAME READY STATUS RESTARTS AGE busybox-7b5f4b66db-4c74c 1/1 Running 0 31d carts-api-extension-79dfb6587b-db5kn 1/1 Running 0 13d carts-api-extension-79dfb6587b-m7wxv 1/1 Running 0 3d carts-api-extension-79dfb6587b-n2wnh 1/1 Running 0 2d21h .... More info about\nEKS auth https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html KUBECONFIG https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/ AWS EKS K8S everyday usage Do AWS auth, which is valid for 8h\naws-google-auth -d 42000 -p juniqe-staging --role-arn arn:aws:iam::445858552116:role/juniqe-staging/admin For production\naws-google-auth -d 42000 -p juniqe-production --role-arn arn:aws:iam::054846004576:role/juniqe-production/operator Set AWS profile, which is valid in current terminal session\nexport AWS_PROFILE=juniqe-staging # for production # export AWS_PROFILE=juniqe-production You are ready to use EKS\nkubectl describe namespace NAME STATUS AGE default Active 11d gitlab Active 11d juniqe-production Active 11d kube-node-lease Active 11d kube-public Active 11d kube-system Active 11d To switch between different K8S cluster, please use kubectl config command\nOperator account has access only to one namespace Pulumi Read general info https://www.pulumi.com/docs/intro/concepts/\nInstall https://pulumi.com/docs/reference/install/\nUsage Go to Pulumi project directory (for example ./aws/eks or ./aws/iam)\ncd ./aws/iam/ Install requirements\nnpm install Make sure you have AWS_PROFILE set up in current session.\nLogin into stack state backend. There are different urls for staging and production. This is similiar to terraform state backend\npulumi login --cloud-url s3://juniqe-staging-pulumi Select stack. Each project contains at lease production and staging stacks\npulumi stack ls pulumi stack select iam-juniqe-staging Check if you are ready to run pulumi\npulumi stack If you want to apply changes, run:\npulumi up --diff Sealed Secrets To protect secrets(passwords, api keys, other critical stuff) in gitlab we should use sealed secrets\nMore info: https://engineering.bitnami.com/articles/sealed-secrets.html\nUsage For example our secret is DATABASE_PASSWORD with value password\nInstall kubeseal\nencode secret value in base64\necho -n \u0026#34;password\u0026#34; | base64 copy result and use in next step\ncreate temp file secret.yaml with secret resource apiVersion: v1 kind: Secret metadata: name: juniqe-app data: DATABASE_PASSWORD: cGFzc3dvcmQ= # \u0026lt; this is base64 encoded `password` Make Sure you are authorized in K8S cluster and have access to it\nCreate Sealed secret resource from secret. Make sure you\u0026rsquo;ve specified right namespace.\ncat secret.yaml | kubeseal -n juniqe-staging --controller-name=secret-sealed-secrets --controller-namespace=kube-system -o yaml Copy output with SealedSecret resource and put it into your k8s resource files. Make sure that you removed createTimestamp and namespace from top-level if it kustomize setup.\napply your resources to k8s (with skaffold or kubectl) as usual\nSealedSecret controller will generate Secret based on your SealedSecret resource\nRemove secret.yaml file from your filesystem\nDeploy your service.\ncheck created secret:\nkubectl get secret juniqe-app -o json | jq .data.DATABASE_PASSWORD -r | base64 -d the answer should be the inital secret value (\u0026ldquo;password\u0026rdquo; in this case)\nIf Secret does not appear, please check SealedSecret controller pod logs\nkubectl -n kube-system logs deploy/sealed-secrets Using this repo This repo consists K8S cluster definition with components. The main cluster lives in aws/eks/ folder and written in pulumi.\nK8S components are using root/component or root/category/component folder structure. Components are written k8s kind definitions, some of them are just downloaded and community managed helm charts. Some components definitions are written by us and using kustomize.\nInstallation of components and cluster are wrapped and unified by invoke.\ninvoke This is python cli tool similiar to make and fabric. invoke supports dependencies between tasks.\ninvoke uses tasks.py file to store tasks. tasks could have dependencies.\nMore info https://www.pyinvoke.org/\nEach component has tasks.py file with default task to install or upgrade component.\nInstalling invoke ArchLinux:\npacman -Sy python-invoke Or on any other distribution by using pip\npip install --user invoke Install \u0026amp; upgrade component or cluster To install any component, go to the folder of component and simply run invoke command. invoke will run default task to install or upgrade component using staging environment. To run on production invoke install --env=juniqe-production\nInstalling everything in the root of this repo, run invoke to run tasks from root tasks.py.\nroot tasks.py consists orchestration over creation \u0026amp; upgrading cluster configuration.\nAdding new commponents Create folder on top level or in category/component Add component via helm or other tool. Wrap installation into invoke\u0026rsquo;s tasks.py Add task with dependencies to the root tasks.py\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/infrastructure-docs/external/hetzner/",
	"title": "Hetzner",
	"tags": [],
	"description": "",
	"content": "About dev.juniqe.com is the Jenkins master server. It is also used as a sandbox for releases by developers. nexus.juniqe.com and aptly.juniqe.com/archive.juniqe.com are also located on this server. dev.juniqe.com Server has 6TB Hard Drive extension mounted as /data Server IP: 138.201.192.111 is whitelisted by many Security Group in AWS\nAWS default access Root user has configured AWS keys created by TF (system-tools/terraform-2.0/providers/aws/global/iam.tf) With access to the AWS ECR Service for the deployments\nnexus.juniqe.com Nexus is our repository for java artifacts, which is used during application build. More details about nexus on official page: https://www.sonatype.com/nexus-repository-oss The web interface is available at https://nexus.juniqe.com/nexus Login credentials can be found in SettingsPlugin.scala Working as a container, initially run with params:\n#first run docker run -d -p 8081:8081 --name nexus -v /data/nexus:/sonatype-work sonatype/nexus #run nexus after server restart docker start nexus #full restart docker rm -f nexus docker run -d -p 8081:8081 --name nexus -v /data/nexus:/sonatype-work sonatype/nexus I don\u0026rsquo;t know any other manual configuration for the nexus. Most likely all configuration changes were made via Web Interface The nginx config for nexus is:\nserver { listen 80; server_name nexus.juniqe.com; return 301 https://$server_name$request_uri; } server { listen 443 ssl; server_name nexus.juniqe.com; error_log /var/log/nginx/nexus.error.log; access_log /var/log/nginx/nexus.access.log; ssl_certificate /home/letsencrypt/juniqe-le.crt; ssl_certificate_key /home/letsencrypt/juniqe-le.key; client_max_body_size 48M; location / { proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://localhost:8081/; } jenkins.juniqe.com enkins master installation - is not automated. It was a manual setup. Some confgirations like SAML Auth or Jenkins for Feature Branches are documented. Besides that Jenkins is regularly update each month to the latest stable version. For the better security all Credentials for the new builds where moved our from plain text in Jobs to the Credentials Plugin So each job can get access to the resources without exposing it to users and logs. More details about Jenkins configuration on this page\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/infrastructure-docs/external/",
	"title": "External",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/infrastructure-docs/maintenance/db/",
	"title": "Data Consistency / DB Maintenance",
	"tags": [],
	"description": "",
	"content": "Extracting products without attributes. It might take up to 30m to complete. Separate database juniqe_data_inconsistency is used to hold results.\nCREATE TABLE catalog_product_entity_without_attributes; SELECT \u0026#34;name\u0026#34; AS \u0026#34;attribute\u0026#34;, cpe.entity_id AS \u0026#34;product_entity_id\u0026#34; FROM juniqe.catalog_product_entity cpe LEFT JOIN juniqe.catalog_product_entity_varchar cpev ON ( cpe.entity_id = cpev.entity_id ) WHERE ( cpev.attribute_id in (SELECT ea.attribute_id FROM juniqe.eav_attribute ea WHERE ( ea.attribute_code = \u0026#34;name\u0026#34; ) ) ) AND ( cpev.value_id IS NULL ) GROUP BY cpe.entity_id UNION SELECT \u0026#34;status\u0026#34; AS \u0026#34;attribute\u0026#34;, cpe.entity_id as \u0026#34;product_entity_id\u0026#34; FROM juniqe.catalog_product_entity cpe LEFT JOIN juniqe.`catalog_product_entity_int` cpei ON ( cpe.entity_id = cpei.entity_id ) WHERE ( cpei.attribute_id IN (SELECT ea.attribute_id FROM juniqe.eav_attribute ea WHERE ( ea.attribute_code = \u0026#34;status\u0026#34; ) ) ) AND ( cpei.value IS NULL ) GROUP BY cpe.entity_id UNION SELECT \u0026#34;url_key\u0026#34; AS \u0026#34;attribute\u0026#34;, cpe.entity_id AS \u0026#34;product_entity_id\u0026#34; FROM juniqe.catalog_product_entity cpe LEFT JOIN juniqe.catalog_product_entity_varchar cpev ON ( cpe.entity_id = cpev.entity_id ) where ( cpev.attribute_id in (select ea.attribute_id from juniqe.eav_attribute ea where ( ea.attribute_code = \u0026#34;url_key\u0026#34; ) ) ) and ( cpev.value_id IS NULL ) group by cpe.entity_id union select \u0026#34;description\u0026#34; as \u0026#34;attribute\u0026#34;, cpe.entity_id as \u0026#34;product_entity_id\u0026#34; from juniqe.catalog_product_entity cpe LEFT JOIN juniqe.catalog_product_entity_text cpet ON ( cpe.entity_id = cpet.entity_id ) where ( cpet.attribute_id in (select ea.attribute_id from juniqe.eav_attribute ea where ( ea.attribute_code = \u0026#34;description\u0026#34; ) ) ) and ( cpet.value_id IS NULL ) group by cpe.entity_id union select \u0026#34;short_description\u0026#34; as \u0026#34;attribute\u0026#34;, cpe.entity_id as \u0026#34;product_entity_id\u0026#34; from juniqe.catalog_product_entity cpe LEFT JOIN juniqe.catalog_product_entity_text cpet ON ( cpe.entity_id = cpet.entity_id ) where ( cpet.attribute_id in (select ea.attribute_id from juniqe.eav_attribute ea where ( ea.attribute_code = \u0026#34;short_description\u0026#34; ) ) ) and ( cpet.value_id IS NULL ) group by cpe.entity_id union select \u0026#34;ready_to_ship_to\u0026#34; as \u0026#34;attribute\u0026#34;, cpe.entity_id as \u0026#34;product_entity_id\u0026#34; from juniqe.catalog_product_entity cpe LEFT JOIN juniqe.`catalog_product_entity_int` cpei ON ( cpe.entity_id = cpei.entity_id ) where ( cpei.attribute_id in (select ea.attribute_id from juniqe.eav_attribute ea where ( ea.attribute_code = \u0026#34;ready_to_ship_to\u0026#34; ) ) ) and ( cpei.value IS NULL ) group by cpe.entity_id union select \u0026#34;ready_to_ship_from\u0026#34; as \u0026#34;attribute\u0026#34;, cpe.entity_id as \u0026#34;product_entity_id\u0026#34; from juniqe.catalog_product_entity cpe LEFT JOIN juniqe.`catalog_product_entity_int` cpei ON ( cpe.entity_id = cpei.entity_id ) where ( cpei.attribute_id in (select ea.attribute_id from juniqe.eav_attribute ea where ( ea.attribute_code = \u0026#34;ready_to_ship_from\u0026#34; ) ) ) and ( cpei.value IS NULL ) group by cpe.entity_id; "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/infrastructure-docs/maintenance/",
	"title": "Maintenance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/maintenance/",
	"title": "Maintenance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/applications/checkout/",
	"title": "Checkout app",
	"tags": [],
	"description": "",
	"content": "Checkout cart staging: https://www.staging.juniqe.com/cm/#/\nproduction: https://www.juniqe.de/cm/\nLogs: https://sentry.io/organizations/juniqe/issues/?project=5272619\nRun checkout tests Run in browser from localhost\nnpm run test:local Run in headless Chrome from localhost\nnpm run test:local:headless Run in browser against Staging\ntest:juniqe-staging npx testcafe chrome qa/tests/functional.js --selector-timeout 50000 --assertion-timeout 20000 --page-load-timeout 15000 -e The preferred domain can be selected by --domain=de or domain=com\nlocal dev with shop integration Start local dev env with shop and nginx-frontend, for example cd ~/workspace/juniqe-infra/docker docker-compose up -d Start checkout app npm install npm start Open http://www.localhost.juniqe.de/cm/#/ nginx-frontend passes all traffic from \u0026ldquo;/cm\u0026rdquo; to localhost:3000/cm\nbuild npm install npm run build:juniqe-staging deploy get aws autorization\nexport AWS_PROFILE=juniqe-staging npm run deploy:juniqe-staging Versioning version is branch(or tag) + sha\nversion is available here: cm/_version.txt\nFor example: https://www.staging.juniqe.com/cm/_version.txt\ninfrastructure checkout app related infratructure is described in infra directory with Pulumi\nTo change infrastructure:\nInstall Pulumi https://www.pulumi.com/docs/get-started/install/ Go to infra subfolder cd infra/ Install pulumi deps: npm install Login to state backed pulumi login --cloud-url s3://juniqe-staging-pulumi Select stack pulumi stack select juniqe-staging-checkout-app To apply changes: pulumi up --diff Monitoring lambda Documentation\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/infrastructure-docs/authentication/aws-sso/",
	"title": "AWS Credentials via SAML (SSO)",
	"tags": [],
	"description": "",
	"content": "If you want to have AWS API access for any script or terraform or even just AWS CLI outside of AWS you need to have a working AWS credential. Making and maintenance of such credentials could be a problem and big security issue. Luckily with login via Google IDP, we don\u0026rsquo;t have anymore permanent keys. The problem that engineers don\u0026rsquo;t have API access. As a solution - we can assumeRoleWith SAML to obtain the AWS STS token. This temporary credentials will work for 1 hour.\nStarting from 0.0.24 longer session (up to 12h) and SAML caching is supported by aws-google-auth\nHow to use aws-google-auth with our docker terraform toolbox Clone or pull last juniqe-infra git@gitlab.com:juniqe/juniqe-infra.git Go to terraform folder cd ~/workspace/juniqe-infra/terraform Make sure you have \u0026ldquo;~/.aws\u0026rdquo; folder with proper rights first # mkdir ~/.aws # touch ~/.aws/config ~/.aws/credentials ~/.aws/saml_cache.xml # ls -la ~/.aws/ total 32 drwx------ 2 maratsh users 4096 Aug 23 14:49 ./ drwx------ 67 maratsh users 4096 Aug 24 12:00 ../ -rw------- 1 maratsh users 2400 Aug 23 16:20 config -rw------- 1 maratsh users 10883 Aug 23 16:20 credentials -rw------- 1 maratsh users 7709 Aug 23 16:20 saml_cache.xml Open ~/.aws/config and create configs per environment. replace profile and username. [profile replace me, in example: juniqe-staging] region = eu-central-1 google_config.ask_role = False google_config.keyring = False google_config.duration = 42000 google_config.google_idp_id = C00wp1uuf google_config.google_sp_id = 86281751805 google_config.u2f_disabled = False google_config.google_username = replace me, in example: marat@juniqe.com [profile replace me, in example: datavirtuality] region = eu-central-1 google_config.ask_role = False google_config.keyring = False google_config.duration = 42000 google_config.google_idp_id = C00wp1uuf google_config.google_sp_id = 86281751805 google_config.u2f_disabled = False google_config.google_username = replace me, in example: marat@juniqe.com Run aws-google-auth from terraform toolbox container. I will ask you for juniqe google password and probably for captcha. Try to open it in your browser. Enter your MFA token or press Yes in Google app on your phone. Now you can select available aws account from dropdown menu by using number. # docker-compose run -e AWS_PROFILE=\u0026#34;replace me, in example: juniqe-staging\u0026#34; --rm terraform aws-google-auth -a -p replace me, in example: juniqe-staging Failed to import U2F libraries, U2F login unavailable. Other methods can still continue. Google Password: https://accounts.google.com/Captcha?v=2\u0026amp;ctoken=AAWk9lSauaGz-ckoVCJ7eSDsnzm4jTwUQB79C4N1jhGKjSgW7r4XIfdEbtM29siisGWnCgFDZkc14R_Srp518xFVRHHR3Afb02jXhoq1-MuLoRsTn3YrqSEVNASVD_nBOJQ9jNcMLUfTdjJ_CylUPJv-NWF7Nu1mOipczySGYwDlH3x1JPwMg58 Captcha (case insensitive): crouseracy Open the Google App, and tap \u0026#39;Yes\u0026#39; on the prompt to sign in ... [ 1] arn:aws:iam::445858552116:role/juniqe-staging/admin [ 2] arn:aws:iam::445858552116:role/juniqe-staging/readonly Type the number (1 - 2) of the role to assume: 7 Assuming arn:aws:iam::445858552116:role/juniqe-staging/admin Credentials Expiration: 2018-08-24 00:31:22+00:00 Check that you successfully authorized # docker-compose run -e AWS_PROFILE=juniqe-staging(replace it with your profile name) --rm terraform aws --profile juniqe-staging(replace it with your profile name) sts get-caller-identity { \u0026#34;UserId\u0026#34;: \u0026#34;AROAI4JI2N52XMITA3AIC:marat@juniqe.com\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;445858552116\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:sts::445858552116:assumed-role/admin/marat@juniqe.com\u0026#34; } Authorize on multiple envs at once Example of the script to get multiple credentials at once (only first call need to be authorized)\nUncomment what you need and place it somewhere in PATH, in example ~/.bin/\nex: juniqe-aws-auth.sh\n#!/bin/bash set -uxe cd ~/workspace/juniqe-infra/terraform docker-compose run --rm terraform aws-google-auth -d 42000 -p juniqe-staging --role-arn arn:aws:iam::445858552116:role/juniqe-staging/readonly docker-compose run --rm terraform aws-google-auth -d 42000 -p juniqe-integration --role-arn arn:aws:iam::067547835157:role/juniqe-integration/readonly docker-compose run --rm terraform aws-google-auth -d 42000 -p juniqe-staging --role-arn arn:aws:iam::445858552116:role/juniqe-staging/admin docker-compose run --rm terraform aws-google-auth -d 42000 -p juniqe-integration --role-arn arn:aws:iam::067547835157:role/juniqe-integration/admin docker-compose run --rm terraform aws-google-auth -d 42000 -p juniqe-production --role-arn arn:aws:iam::054846004576:role/juniqe-production/admin docker-compose run --rm terraform aws-google-auth -d 42000 -p juniqe-main --role-arn arn:aws:iam::603482864741:role/juniqe/admin Examples of running all commands should be running workspace/juniqe-infra/terraform\nInit ( vendors and state download ), plan (dry-run) and apply(actual run) cms on juniqe-staging # Init cms-app layer: docker-compose run -e AWS_PROFILE=\u0026#34;juniqe-staging\u0026#34; -w /tf/envs/juniqe-staging/cms-app --rm terraform tf init # plan layer with variable version=1.0.0: docker-compose run -e AWS_PROFILE=\u0026#34;juniqe-staging\u0026#34; -w /tf/envs/juniqe-staging/cms-app --rm terraform tf plan -var version=1.0.0 # Apply layer docker-compose run -e AWS_PROFILE=\u0026#34;juniqe-staging\u0026#34; -w /tf/envs/juniqe-staging/cms-app --rm terraform tf apply -var version=1.0.0 Run terragrunt plan on whole juniqe-integration infrastructure via terragrunt. docker-compose run -e AWS_PROFILE=\u0026#34;juniqe-integration\u0026#34; --rm terraform terragrunt plan-all --terragrunt-non-interactive --terragrunt-working-dir /tf/envs/juniqe-integration get all instances on juniqe-staging via aws cli. docker-compose run -e AWS_PROFILE=\u0026#34;juniqe-staging\u0026#34; terraform aws ec2 describe-instances "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/lambdas/image-resize/lambdas/",
	"title": "AWS Lambda for image rendering",
	"tags": [],
	"description": "",
	"content": "images-s3-cdn (terraform/modules/lambda-image-rendering/images-s3-cdn)\nThere are 3 main buckets involved in our rendering and resizing operations We use these DNS entries to serve images directly (through a Cloudfront distribution) from S3.\nrndr.juniqe.com rndr.juniqe-staging.juniqe.com rndr.juniqe-integration.juniqe.com juniqe-design-cloud This bucket is in the main account\nWe store all high-res printing images in this bucket. Images are organized in verticals and by designer like\njuniqe-design-cloud/Wall Art/06 - Chase Kunz/6-1-101L-40x60.jpg juniqe-design-cloud/Wall Art/06 - Chase Kunz/6-1-101L-20x30.jpg juniqe-design-cloud/Living/Bed Linen/11 - Leah Flores/11-3-JB01-140x200.jpg \u0026hellip;. juniqe-shop-cloud bucket This bucket is in the main account\nDuring upload into the designcloud where we generate all possible smaller sized for an uploaded image like 6-1-101L-40x60.jpg we upload a lower-res image (72 × 72 ppi) into the juniqe-shop-cloud of the largest possible size.\nWe store images in the juniqe-shop-cloud in the identical bucket/prefix structure as in the juniqe-design-cloud.\nThe juniqe-shop-cloud will most likely become obsolete once we move to an on-demand rendering approach of both shop images and printing images)\njuniqe-shop-cloud/Wall Art/06 - Chase Kunz/6-1-101L-40x60.jpg (only the largest possible size) \u0026hellip;. juniqe-shop-images This is the main shop-image bucket that is configured for static content hosting and referenced by https://rndr.juniqe.com/media/.. Images that we upload through our cms-app to render on arbitrary cms-block and the like are stored in this bucket as well as all product images.\nFor staging and integration we are using\njuniqe-staging-shop-images juniqe-integration-shop-images The folder structure follows the magento legacy folder structure.\njuniqe-shop-images/media/catalog/product/xxx juniqe-shop-images/media/catalog/category/xxx \u0026hellip;. We are rsyncing from production (juniqe-shop-images) to staging and to integration the images that are not generated by the lambda functions.\nAWS lambda code (terraform/modules/juniqe/lambda-image-rendering/lambda, terraform/modules/juniqe/lambda-image-rendering/product-image-rendering, terraform/modules/juniqe/lambda-image-rendering/product-image-resize, terraform/modules/juniqe/lambda-image-rendering/template-cache-cleanup, terraform/modules/juniqe/lambda-image-rendering/template-image-rendering, terraform/modules/juniqe/lambda-image-rendering/template-rendering-assembly)\nWe currently maintain our AWS lambda codebase in 2 repositories.\ngit@bitbucket.org:juniqe/lambda-template-rendering.git (scala) git@bitbucket.org:juniqe/lambda-image-resize.git (python) Our view in room rendering is implemented in Scala because we execute some SKU parsing in the templated rendering.\nhttps://rndr.juniqe.com/template/view_in_room20/x386/c547dacb974435434a9a53a85d2f2ca58cdf1991bbb821e4d3046acad2e42b75/sku_221-121-101P-80x120/view_in_room.jpg\nThe cache resize and seo optimized cache resize lambda functions are implemented in python because they are very simple and just not a lot of domain specific code.\nhttps://rndr.juniqe.com/media/catalog/product/cache/x800/221/121/221-121-101P.jpg https://rndr.juniqe.com/media/catalog/product/seo-cache/x386/675/51/675-51-101P/Frida-treechild-Premium-Poster.jpg\napi-gateway (terraform/modules/juniqe/lambda-image-rendering/api-gateway)\nWith API Gateway we implemented all path mappings. We use the DNS entries\nrndr-api.juniqe.com rndr-api.juniqe-staging.juniqe.com rndr-api.juniqe-integration.juniqe.com These are the URLs that we redirect to upon a 404 on the juniqe-shop-images bucket(s).\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/infrastructure-docs/cloud/cross-account-peering/",
	"title": "AWS private zone cross-account peering",
	"tags": [],
	"description": "",
	"content": "HOWTO Requirements: AWS Account or Role access. aws-google-auth installed docker and docker-compose installed. Git installed. It\u0026rsquo;s possible to resolve internal zone records from one vpc to another in different account. For example: We have the account juniqe-production with zone juniqe-production (zone id ZONEIDJPR) and juniqe-main account with vpc vpc-7main; The goal is to make zone juniqe-production resolvable inside juniqe-main. To do so:\nGet the repository juniqe-infra for Gitlab: git clone https://gitlab.com/juniqe/juniqe-infra.git then, and go to terraform directory: cd terraform Get authorization for aws profile juniqe-main and juniqe-production. aws-google-auth -d 42000 -a -p juniqe-production aws-google-auth -d 42000 -a -p juniqe-main Authorize juniqe-main vpc to associate with zone juniqe-production in juniqe--production account. docker-compose run -e AWS_PROFILE=\u0026#34;juniqe-production\u0026#34; --rm terraform aws route53 create-vpc-association-authorization --hosted-zone-id ZONEIDJPR --vpc \u0026#34;VPCRegion=eu-central-1,VPCId=vpc-7main\u0026#34; { \u0026#34;HostedZoneId\u0026#34;: \u0026#34;ZONEIDJPR\u0026#34;, \u0026#34;VPC\u0026#34;: { \u0026#34;VPCRegion\u0026#34;: \u0026#34;eu-central-1\u0026#34;, \u0026#34;VPCId\u0026#34;: \u0026#34;vpc-7main\u0026#34; } } Create association in juniqe-main account. docker-compose run -e AWS_PROFILE=\u0026#34;jsmain\u0026#34; --rm terraform aws route53 associate-vpc-with-hosted-zone --hosted-zone-id ZONEIDJPR --vpc \u0026#34;VPCRegion=eu-central-1,VPCId=vpc-7main\u0026#34; { \u0026#34;ChangeInfo\u0026#34;: { \u0026#34;Id\u0026#34;: \u0026#34;/change/C2S4FIJCTH1OEL\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;PENDING\u0026#34;, \u0026#34;SubmittedAt\u0026#34;: \u0026#34;2018-07-06T14:40:41.032Z\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;\u0026#34; } } Done; Check the DNS inside juniqe-main; Authority section should be filled. dig juniqe-production ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.13.0 \u0026lt;\u0026lt;\u0026gt;\u0026gt; juniqe-production ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 45662 ;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;juniqe-production. IN A ;; AUTHORITY SECTION: juniqe-production. 60 IN SOA ns-1536.awsdns-00.co.uk. awsdns-hostmaster.amazon.com. 1 7200 900 1209600 86400 ;; Query time: 21 msec ;; SERVER: 172.31.0.2#53(172.31.0.2) ;; WHEN: Fri Jul 06 16:41:29 CEST 2018 ;; MSG SIZE rcvd: 133 Usefull links AWS reference article: Associating an Amazon VPC and a private hosted zone that you created with different AWS accounts Terraform support: Resource: aws_route53_zone_association terraform ticket about support https://github.com/hashicorp/terraform/issues/12465 tf helper module to automate this thing https://github.com/opetch/terraform-aws-cli-resource "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/onboarding/aws-sso-google-saml/",
	"title": "AWS SSO via Google SAML",
	"tags": [],
	"description": "",
	"content": "Many information is available in Internet and detailed manual configuration described in AWS Blog At JUNIQE configuration made via TF for each account.\nAWS config The main AWS account configuration at the moment located at:\nhttps://bitbucket.org/juniqe/system-tools/src/master/terraform-2.0/providers/aws/global/main.tf?at=master\u0026amp;fileviewer=file-view-default With one resource description: # SAML Providerresource \u0026quot;aws_iam_saml_provider\u0026quot; \u0026quot;juniqe\u0026quot; { name = \u0026quot;GSuite\u0026quot; saml_metadata_document = \u0026quot;${file(\u0026quot;files/saml-metadata.xml\u0026quot;)}\u0026quot;} After this we create a roles which is mapping in Google Admin with custom schema.\nGoogle config In section of Apps \u0026gt; SAML Apps is always available the latest metadata for the Google IdP which is used in terraform.\nCustom Schema We need to build a mapping for users in Google and AWS. For this one better to extend existing schema with custom fields. That is the Schema used for user:\n{ \u0026#34;fields\u0026#34;: [ { \u0026#34;fieldName\u0026#34;: \u0026#34;role\u0026#34;, \u0026#34;fieldType\u0026#34;: \u0026#34;STRING\u0026#34;, \u0026#34;readAccessType\u0026#34;: \u0026#34;ADMINS_AND_SELF\u0026#34;, \u0026#34;multiValued\u0026#34;: true }, { \u0026#34;fieldName\u0026#34;: \u0026#34;duration\u0026#34;, \u0026#34;fieldType\u0026#34;: \u0026#34;INT64\u0026#34;, \u0026#34;readAccessType\u0026#34;: \u0026#34;ADMINS_AND_SELF\u0026#34; } ], \u0026#34;schemaName\u0026#34;: \u0026#34;SSO\u0026#34; } Could be added via developers.google.com: schemas/insert\nMapping between AWS and Google. 2 options is required, but with 3rd option we can extend the session time.\nAPP setting Provider section Provider attribute https://aws.amazon.com/SAML/Attributes/RoleSessionName Basic Information. Primary Email * https://aws.amazon.com/SAML/Attributes/Role SSO. role ** https://aws.amazon.com/SAML/Attributes/SessionDuration SSO. duration * this attribute is responsible what name of user will be in the AWS logs\n** that is the list of available AWS Roles for user\nAssign user via Google Admin Via the Google Admin panel is possible to add/change all custom attributes. As an example:\nThe role attribute is consist of \u0026ldquo;,\u0026rdquo;. Provider ARN changes per AWS account (and provider but we are using only one at the moment) Assign user, alternatives It could be done with developers.goohle.com users/patch, or any custom script, which changes permissions for users. But via Google Admin it is easier anyway. Here an example of payload with all roles for our accounts at the moment.\n{ \u0026#34;customSchemas\u0026#34;: { \u0026#34;SSO\u0026#34;: { \u0026#34;duration\u0026#34;: 43200, \u0026#34;role\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;arn:aws:iam::603482864741:role/juniqe/admin,arn:aws:iam::603482864741:saml-provider/google\u0026#34;, \u0026#34;customType\u0026#34;: \u0026#34;JUNIQE Main Admin\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;arn:aws:iam::603482864741:role/juniqe/readonly,arn:aws:iam::603482864741:saml-provider/google\u0026#34;, \u0026#34;customType\u0026#34;: \u0026#34;JUNIQE Main ReadOnly\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;arn:aws:iam::530590334745:role/juniqe-datavirtuality/admin,arn:aws:iam::530590334745:saml-provider/google\u0026#34;, \u0026#34;customType\u0026#34;: \u0026#34;DataVirtuality Admin\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;arn:aws:iam::530590334745:role/juniqe-datavirtuality/readonly,arn:aws:iam::530590334745:saml-provider/google\u0026#34;, \u0026#34;customType\u0026#34;: \u0026#34;DataVirtuality ReadOnly\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;arn:aws:iam::097140904496:role/snowplow-juniqe/admin,arn:aws:iam::097140904496:saml-provider/google\u0026#34;, \u0026#34;customType\u0026#34;: \u0026#34;SnowPlow Admin\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;arn:aws:iam::097140904496:role/snowplow-juniqe/readonly,arn:aws:iam::097140904496:saml-provider/google\u0026#34;, \u0026#34;customType\u0026#34;: \u0026#34;SnowPlow ReadOnly\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;arn:aws:iam::445858552116:role/juniqe-staging/admin,arn:aws:iam::445858552116:saml-provider/google\u0026#34;, \u0026#34;customType\u0026#34;: \u0026#34;juniqe-staging Admin\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;arn:aws:iam::445858552116:role/juniqe-staging/readonly,arn:aws:iam::445858552116:saml-provider/google\u0026#34;, \u0026#34;customType\u0026#34;: \u0026#34;juniqe-staging ReadOnly\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;arn:aws:iam::603482864741:role/juniqe/designers,arn:aws:iam::603482864741:saml-provider/google\u0026#34;, \u0026#34;customType\u0026#34;: \u0026#34;JUNIQE Main designers\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;arn:aws:iam::603482864741:role/juniqe/finance,arn:aws:iam::603482864741:saml-provider/google\u0026#34;, \u0026#34;customType\u0026#34;: \u0026#34;JUNIQE Main finance\u0026#34; } ] } } } "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/infrastructure-docs/bi-infrastructure/",
	"title": "BI infrastructure",
	"tags": [],
	"description": "",
	"content": " Matilion: https://gitlab.com/juniqe/infra/cloud/-/tree/master/aws/bi Additional: https://gitlab.com/juniqe/juniqe-infra/-/tree/master/terraform/envs/datavirtuality Snowplow: The separate account managed by the snowplow itself "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/first-content/",
	"title": "Cloud &amp; Infrastructure",
	"tags": [],
	"description": "",
	"content": " Environments We run our servers in different environments (separate AWS accounts). These are:\njuniqe-production juniqe-staging juniqe-integration (currently not in use) Deployments We follow the following motos for all our deployments:\nApplication infrastructure should be managed and deployed as part of the application code. All deployments should be implemented with CI/CD in mind. FreatureBranching should be automated and be part of the CI/CD. Any change to our applications is currently going through the following steps:\nFeature branch creation. Feature implementation. Once the MR is created the CI is triggered for the MR. Code Review. QA (as needed). CD once the MR is approved by the Code Owner and the QA. Critical Services. The Juniqe Infrastructure is composed of several services, including the lambdas. In all those services, exist some that are more critical than others, this is the list of them:\ncarts-API-extension: (add to cart API response). storage (ELK stack where the index of the products are hosted). shop (the main service of the shop). Magento (backend of the shop). PDP (the service that allows the customer to customize the products). "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/onboarding/providers/cognito/",
	"title": "Cognito",
	"tags": [],
	"description": "",
	"content": " Cognito: no actions are needed "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/devops-entrypoints/",
	"title": "DevOps Entrypoints",
	"tags": [],
	"description": "",
	"content": "New accounts [on/off] boarding VPN management: Production VPN MySQL users: SSO: Providers Cognito - no actions are needed. Commercetools MC SSO: CommerceTools SSO. Google Groups. On GSuite new Engineering accounts should be added to these groups:\nAll Engineering For DevOps also the group: DevOps GitLab and Sentry: access should be granted to the projects\nInfrastructure management New infrastructure on K8s is documented in: cloud repository\nEverything around K8s DNS management for the cluster K8s IAM ALB for K8s BI infrastructure The old part of the infrastructure is in: juniqe-infra\nEnv creation \u0026amp; global configs VPC \u0026amp; Peering Frontend LB Emarsys subdomains DNS outside of K8s Certificates outside of K8s RDS DB Elastic Caches Other team support services Local setup \u0026amp; docker containers Production to staging data migration:\nMySQL https://gitlab.com/juniqe/juniqe-infra/-/tree/master/docker/infra/rds-manager https://gitlab.com/juniqe/juniqe-infra/-/blob/master/terraform/modules/juniqe/rds-master/main.tf#L25 https://gitlab.com/juniqe/juniqe-infra/-/blob/master/terraform/modules/juniqe/rds-master/main.tf#L17\nES https://gitlab.com/juniqe/infra/storage https://gitlab.com/juniqe/infra/storage/-/blob/master/k8s/base/curatorJobs.yaml The backups are stored in S3 we should implement a retention policy for the backups.\nVPC Peering https://gitlab.com/juniqe/juniqe-infra/-/blob/master/terraform/envs/juniqe-staging/vpc/production_vpc_peering.tf https://gitlab.com/juniqe/juniqe-infra/-/blob/master/terraform/envs/juniqe-production/vpc/staging_vpc_peering.tf https://app.clickup.com/4580628/v/dc/4bt8m-736/4bt8m-47881\nDNS Management The DNS management for all shop domains is done as part of the PDP app, in: https://gitlab.com/juniqe/pdp-app/-/tree/master/dns\nThe configuration of the DNS per application is done as part of the app infra.\nRoute53/Domain management is part of the juniqe-main account and is managed manually.\nWe also have 2 zones: prod.juniqe.com and stage.juniqe.com that\u0026rsquo;s managed in: https://gitlab.com/juniqe/infra/cloud/-/tree/master/external-dns\nEmail sub-domains are managed in terraform: https://gitlab.com/juniqe/juniqe-infra/-/tree/master/terraform/envs/juniqe/marketing\nWeak points and caution zones: nginx-frontend and CloudFront\nLogging: Logging\nK8s IAM policy injection https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html https://gitlab.com/juniqe/logistics/-/blob/master/infra/logistic-app/iam.ts https://gitlab.com/juniqe/logistics/-/blob/master/k8s/base/resources/LogisticApp.yaml#L49 https://gitlab.com/juniqe/logistics/-/blob/master/k8s/base/resources/LogisticAppSA.yaml\nLoad Balancers: Controller: https://gitlab.com/juniqe/infra/cloud/-/tree/master/aws/aws-load-balancer-controller\nCDN Storefront CDN: https://gitlab.com/juniqe/pdp-app/ Image resize: Image resize:\nhttps://gitlab.com/juniqe/juniqe-infra/-/tree/master/terraform/modules/juniqe/lambda-image-rendering https://juniqe.atlassian.net/wiki/spaces/EN/pages/50757697/Rendering+API https://juniqe.atlassian.net/wiki/spaces/EN/pages/504135681/Rendering+Sequence https://bitbucket.org/juniqe/juniqe-infra/src/SYS-746-terraform-our-lambdaapi-gateway-/terraform/envs/README.md Checkout: https://gitlab.com/juniqe/checkout-app/\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/infrastructure-docs/external-infrastructure/",
	"title": "External infrastructure",
	"tags": [],
	"description": "",
	"content": "Hetzner hetzner\nExternal FTP servers:\nOPS FTP services: https://app.clickup.com/4580628/v/dc/4bt8m-736/4bt8m-47921 Marketing: https://app.clickup.com/4580628/v/dc/4bt8m-736/4bt8m-6861 "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/onboarding/providers/google/",
	"title": "Google",
	"tags": [],
	"description": "",
	"content": "Google Groups: On GSuite new Engineering accounts should be added to the groups: All and Engineering For DevOps also the group DevOps\nGitLab and Sentry: access should be granted to the projects\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/onboarding/magento/",
	"title": "Magento Documentation",
	"tags": [],
	"description": "",
	"content": "Magento back office Go to admin.juniqe.com Open system → permissions → users → add new user Fill the form, using gmail user as username. "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/onboarding/providers/merchant-center-sso/",
	"title": "Merchant Center SSO",
	"tags": [],
	"description": "",
	"content": "We do use SSO with Juniqe Google Apps user database. Auth0 is used as OIDC provider.\nhttps://docs.commercetools.com/merchant-center/single-sign-on-beta\nUsage Open https://mc.commercetools.com/login/sso Enter organisation name \u0026ldquo;juniqe\u0026rdquo; In Auth0 opening dialog, click \u0026ldquo;Continue with JuniqeGoogleApps\u0026rdquo; In Sign in with Google, sign in into juniqe google account(gmail) You are in! new users have no privilgies, Administrator supposed to invite them to proper groups after first login\nSetup for new CT Org Setup G Suite Enterprise connection in Auth0 (follow Auth0 instructions). If there are one, we don\u0026rsquo;t need new one. Setup Application in Auth0. We need 1 app per org. Create CT team new users with no privilegies Fill in Commerce tools SSO page with Auth0 config. Example\nAuthority URL: https://ichbinjuniqe.eu.auth0.com/oauth/ Client ID: client id from auth0 app Team ID: new users "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/onboarding/mysql-users/",
	"title": "MySQL users onboarding",
	"tags": [],
	"description": "",
	"content": "Read access to production mysql database [this is outdated, the new process is different] The form should be filled https://goo.gl/forms/lC5JMNm3GsogmOlW2\nProvide access via connecting to master and run commands; Choose a strong password.\nmysql -vv -h masterdb.juniqe-production. -u master -p mysql\u0026gt; CREATE USER \u0026#39;ro_firstname_lastname\u0026#39;@\u0026#39;172.31.13.38\u0026#39; IDENTIFIED BY \u0026#39;CHANGE_TO_SECURE_PASSWORD\u0026#39;; mysql\u0026gt; GRANT select,usage,show view on juniqe.* TO \u0026#39;ro_firstname_lastname\u0026#39;@\u0026#39;172.31.13.38\u0026#39; ; simple script to automate account creation Expand source:\n#!/bin/bash set -ue -o pipefail function q() { mysql -vv -h masterdb.juniqe-production -D juniqe -u master -p -e \u0026#34;${1}\u0026#34; } firstname=\u0026#34;${1}\u0026#34; lastname=\u0026#34;${2}\u0026#34; # email=\u0026#34;${3}\u0026#34; full_username=\u0026#34;ro_${firstname}_${lastname:0:1}\u0026#34; username=${full_username:0:16} allowed_ip=\u0026#34;172.31.13.38\u0026#34; # allowed_ip=\u0026#34;10.0.101.179\u0026#34; pwd_data=\u0026#34;$(curl -s -q -d \u0026#39;secret=SECRET\u0026amp;ttl=604800\u0026#39; -u \u0026#39;marat@juniqe.com:\u0026lt;api token\u0026gt;\u0026#39; https://onetimesecret.com/api/v1/generate)\u0026#34; password=\u0026#34;$(echo $pwd_data | jq -r \u0026#34;.value\u0026#34; )\u0026#34; password_link=\u0026#34;https://onetimesecret.com/secret/$(echo $pwd_data | jq -r \u0026#34;.secret_key\u0026#34;)\u0026#34; echo \u0026#34;${username}\u0026#34; | head -c 16 | wc q \u0026#34; BEGIN; CREATE USER \u0026#39;${username}\u0026#39;@\u0026#39;${allowed_ip}\u0026#39; IDENTIFIED BY \u0026#39;${password}\u0026#39;; GRANT select,usage,show view on juniqe.* TO ${username}@${allowed_ip}; COMMIT; \u0026#34; mysql -vv -h masterdb.juniqe-production -D juniqe -u\u0026#34;${username}\u0026#34; -p${password} -e \u0026#34;select 1;\u0026#34; message=\u0026#34; This is your access to juniqe database Host: \u0026#34;slavedb.juniqe.com\u0026#34; Port: 3306 Username: ${username} Password: Open this link to get it ${password_link} (This link will expire after 7 days) VPN should be activated before mysql connection \u0026#34; echo \u0026#34;${message}\u0026#34; Provide connection information to user via secure channel.\nProvide connection information to user via secure channel. Host: slavedb.juniqe-production Username: ro_firstname_lastname Password: \u0026#39;CHANGE_TO_SECURE_PASSWORD\u0026#39; Database: juniqe Port: 3306 VPN should be activated in order to use it "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/infrastructure-docs/terraform/",
	"title": "Terraform usage",
	"tags": [],
	"description": "",
	"content": "Before you start Make sure you have done this how-to\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/onboarding/vpn/",
	"title": "VPN onboarding",
	"tags": [],
	"description": "",
	"content": "Openvpn server This image is containing all data, including ca cert and server crt/key to run jumphost. aws s3 bucket juniqe-pki is saving state of easy rsa pki In order to run this image we need ec2 instance to run docker. Image for this instance is build by packer and contains script to update container image on schedule\nGenerate new client pull latest juniqe-infra Authorize in account juniqe as admin Authorize in main account docker container repository Get the ca.key pass Go to openvpn container definition cd ~/workspace/juniqe-infra/docker/infra/openvpn-server Run client config generation via script with juniqe email as first argument. The script will request password for ca.key # /bin/bash create_client_config.sh \u0026lt;username\u0026gt;@juniqe.com Get the client conf file from openvpn-data/clients/jumphost02.juniqe.com-@juniqe.com.ovpn Revoke https://github.com/kylemanna/docker-openvpn/blob/1228577d4598762285958ad98724ab37e7b11354/docs/clients.md#revoking-client-certificates\nreconfiguring container Read this doc about openvpn docker container https://github.com/kylemanna/docker-openvpn#quick-start\nSynchronize the pki if CA interaction needed\nbash download_pki_data.sh Change configs in openvpn-data/conf\nChange ovpn_env.sh to configure server config, as openvpn.conf is generated config Change dynamic_routes to change which dns records will be queried for client route push Run docker build\ndocker-compose build Test settings locally\nRun vpn server docker-compose down -v # ensure old container and volumes are discarded docker-compose up Setup client config for pointing to 127.0.0.1 as server Run local client against local server Publish new image with new settings\nauthorize in ekr docker-compose push Wait 1-2 minutes for updating container on running vpn ec2 server Commit changes\nsync PKI changes if they were made\nbash upload_pki_data.sh Reconfigure ec2 instance image Change configs in jumphost-config Run packer packer build packer.json update running instance Replace ami id in ~/workspace/juniqe-infra/envs/juniqe-staging/vpn/main.tf apply changes cd ~/workspace/juniqe-infra/envs/juniqe-staging/vpn terraform apply commit changes "
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/infra/infrastructure-docs/authentication/mutiple-auth/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/posts/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Engineering index\n"
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://juniqe.gitlab.io/devops/docs/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]